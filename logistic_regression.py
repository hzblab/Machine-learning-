# -*- coding: utf-8 -*-
"""Logistic regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FTDJDyK5Op56qktIg4AUtMAoMPBDNIMB
"""

from sklearn.datasets import load_breast_cancer
import numpy as np

data = load_breast_cancer()
X = data.data
y = data.target

print(X.shape)
print(y.shape)

# 2) Train/test split (80/20)
n = X.shape[0]
rng = np.random.default_rng(42)
indices = rng.permutation(n)
split = int(0.8 * n)

train_idx = indices[:split]
test_idx  = indices[split:]

X_train, X_test = X[train_idx], X[test_idx]
y_train, y_test = y[train_idx], y[test_idx]

print("X_train:", X_train.shape, "X_test:", X_test.shape)
print("y_train:", y_train.shape, "y_test:", y_test.shape)

# 3) Standardize using TRAIN only
mu = X_train.mean(axis=0)          # (d,)
sigma = X_train.std(axis=0)        # (d,)

# Guard against constant columns
sigma_safe = sigma.copy()
sigma_safe[sigma_safe == 0] = 1.0

X_train_s = (X_train - mu) / sigma_safe
X_test_s  = (X_test  - mu) / sigma_safe

print("Any NaN in X_train_s?", np.isnan(X_train_s).any(), "Any inf?", np.isinf(X_train_s).any())
print("Any NaN in X_test_s?",  np.isnan(X_test_s).any(),  "Any inf?", np.isinf(X_test_s).any())

def sigmoid(z):
    g = 1/(1+np.exp(-z))
    return g

w = np.zeros(X_train_s.shape[1])
b = 0.0
z = X_train_s @ w + b
y_hat = sigmoid(z)
print(y_hat[:5])
print(y_hat.shape)

def compute_cost_logistic(X_train_s, y_train, w, b):
    cost = 0.0
    z = X_train_s @ w + b
    y_hat = sigmoid(z)
    eps = 1e-15
    y_hat = np.clip(y_hat, eps, 1-eps)
    cost = -np.mean(y_train * np.log(y_hat) + (1 - y_train) * np.log(1 - y_hat))
    return cost

c = compute_cost_logistic(X_train_s, y_train, w, b)
print(c)

def gradient(X_train_s, y_train, w, b):
  m = X_train_s.shape[0]
  z = X_train_s @ w + b
  y_hat = sigmoid(z)
  error = y_hat - y_train
  dw = 1/m * X_train_s.T @ error
  db = 1/m * np.sum(error)
  return dw, db

w = np.zeros(X_train_s.shape[1])
b = 0.0
dw, db = gradient(X_train_s, y_train, w, b)
print(dw.shape)
print(db)

def fit_logreg(X_train_s, y_train, lr=0.1, iters=2000):
  w=np.zeros(X_train_s.shape[1])
  b=0.0
  losses = []
  for i in range(iters):
    dw, db = gradient(X_train_s, y_train, w, b)
    w -= lr * dw
    b -= lr * db
    if i % 100 == 0:
      losses.append(compute_cost_logistic(X_train_s, y_train, w, b))
  return w, b, losses

w, b, losses = fit_logreg(X_train_s, y_train, lr=0.1, iters=2000)
print("first 3:", losses[:3])
print("last 3:", losses[-3:])

p_test = sigmoid(X_test_s @ w + b)  # shape (m_test,)

def f1_at_threshold(y_true, p, t):
    pred = (p >= t).astype(int)


    tp = np.sum((y_true==1) & (pred==1))
    fp = np.sum((y_true==0) & (pred==1))
    fn = np.sum((y_true==1) & (pred==0))


    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
    recall    = tp / (tp + fn) if (tp + fn) > 0 else 0.0


    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0
    return f1, precision, recall

thresholds = np.linspace(0.0, 1.0, 101)

best_t = None
best_f1 = -1.0
best_pr = None
best_rc = None

for t in thresholds:
    f1, pr, rc = f1_at_threshold(y_test, p_test, t)
    if f1 > best_f1:
        best_f1 = f1
        best_t = t
        best_pr = pr
        best_rc = rc

f1_05, pr_05, rc_05 = f1_at_threshold(y_test, p_test, 0.5)

print("Best threshold:", best_t)
print("Best F1:", best_f1, "Precision:", best_pr, "Recall:", best_rc)
print("At t=0.5  F1:", f1_05, "Precision:", pr_05, "Recall:", rc_05)