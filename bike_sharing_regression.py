# -*- coding: utf-8 -*-
"""Bike Sharing regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_KABmJrTJSvfRsqhfg87ofhESAcOvmdA
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

df = pd.read_csv(path)
df.head(5)

import numpy as np

data = df.to_numpy()
# Features: exclude 'instant' and 'dteday' (first 2 cols), and exclude last col (target)
X = data[:, 2:-1]
# Target: last column, keep 2D (n, 1)
y = data[:, -1].astype(float).reshape(-1, 1)

# Ensure X is numeric
X = X.astype(float)

print("X shape:", X.shape, "y shape:", y.shape)

# 2) Train/test split (80/20)
n = X.shape[0]
rng = np.random.default_rng(42)
indices = rng.permutation(n)
split = int(0.8 * n)

train_idx = indices[:split]
test_idx  = indices[split:]

X_train, X_test = X[train_idx], X[test_idx]
y_train, y_test = y[train_idx], y[test_idx]

print("X_train:", X_train.shape, "X_test:", X_test.shape)
print("y_train:", y_train.shape, "y_test:", y_test.shape)

# 3) Standardize using TRAIN only
mu = X_train.mean(axis=0)          # (d,)
sigma = X_train.std(axis=0)        # (d,)

# Guard against constant columns
sigma_safe = sigma.copy()
sigma_safe[sigma_safe == 0] = 1.0

X_train_s = (X_train - mu) / sigma_safe
X_test_s  = (X_test  - mu) / sigma_safe

print("Any NaN in X_train_s?", np.isnan(X_train_s).any(), "Any inf?", np.isinf(X_train_s).any())
print("Any NaN in X_test_s?",  np.isnan(X_test_s).any(),  "Any inf?", np.isinf(X_test_s).any())

# 4) Linear regression (GD)
def predict(X_input, w, b):
    # X_input: (n, d), w: (d, 1), b: scalar
    return X_input @ w + b

def mse(y_hat, y_true):
    # both (n, 1)
    return np.mean((y_hat - y_true) ** 2)

def gradient(X_input, y_true, w, b):
    n = X_input.shape[0]
    y_hat = predict(X_input, w, b)
    err = y_hat - y_true                       # (n, 1)
    dw = (2 / n) * (X_input.T @ err)           # (d, 1)
    db = (2 / n) * np.sum(err)                 # scalar
    return dw, db

def train_gd(X_input, y_true, alpha=0.01, epochs=5000, log_every=100):
    d = X_input.shape[1]
    w = np.zeros((d, 1))
    b = 0.0
    losses = []

    for i in range(epochs):
        dw, db = gradient(X_input, y_true, w, b)
        w -= alpha * dw
        b -= alpha * db

        if log_every and i % log_every == 0:
            losses.append(mse(predict(X_input, w, b), y_true))

    return w, b, np.array(losses)

# Train
w_hat, b_hat, losses_his = train_gd(X_train_s, y_train, alpha=0.01, epochs=5000, log_every=100)
print(w_hat, b_hat, w_hat.shape, losses_his)

# 5) Evaluate
import matplotlib.pyplot as plt
train_mse = mse(predict(X_train_s, w_hat, b_hat), y_train)
test_mse  = mse(predict(X_test_s,  w_hat, b_hat), y_test)

print("Train MSE:", train_mse)
print("Test  MSE:", test_mse)

# Plot training loss history
iters = np.arange(len(losses_his)) * 100
plt.figure()
plt.plot(iters, losses_his)
plt.xlabel("Iteration")
plt.ylabel("MSE (train)")
plt.title("Training Loss (MSE) over Iterations")
plt.show()