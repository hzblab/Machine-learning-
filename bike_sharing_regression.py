# -*- coding: utf-8 -*-
"""Bike sharing regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zvUByZ26flgPmm3LdjM7IoYE9eN_qYlm
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

df = pd.read_csv(path)
df.head(5)

# Convert DataFrame to NumPy array
data = df.to_numpy()
# Features (all columns except last)
X = data[:, 2:-1]
# Target (last column)
y = data[:, 15:].astype(float)
print (X.shape, y.shape)

import numpy as np

# Number of samples
n = X.shape[0]
np.random.seed(42)
indices = np.random.permutation(n)

# Split index
split = int(0.8 * n)

# Split indices
train_idx = indices[:split]
test_idx = indices[split:]

# Create splits
X_train = X[train_idx]
X_test  = X[test_idx]
y_train = y[train_idx]
y_test  = y[test_idx]

# Check shapes
print(X_train.shape, X_test.shape)
print(y_train.shape, y_test.shape)

X_train_num = X_train.astype(float)
mu = X_train_num.mean(axis=0)
sigma = X_train_num.std(axis=0)
X_train_scaled = (X_train_num - mu) / sigma
print(X_train_scaled.shape)

def predict(X_input,  w, b):
  return X_input @ w + b

def mse(y_hat, y_train):
  return np.mean(y_hat - y_train)**2

w = np.zeros((X.shape[1], 1))
b = 0.0

y_hat = predict(X_train_scaled, w, b)
print(mse(y_hat, y_train))

def gradient(X_train_scaled, y_train, w, b):
  n = X_train_scaled.shape[0]
  y_hat = predict(X_train_scaled, w, b) # Calculate y_hat for current w, b
  err = y_hat - y_train
  dw0 = 2/n * X_train_scaled.T @ err # Corrected syntax
  db0 = 2/n * np.sum(err)
  return dw0, db0

dw, db = gradient(X_train_scaled, y_train, w, b)
print(dw, db, dw.shape)

alpha = 0.01
w1 = w - alpha * dw
b1  = b - alpha * db

y_hat1 = predict(X_train_scaled, w1, b1)
loss1 = mse(y_hat1, y_train)
print(w1, b1, loss1)

def train_god(X_train_scaled, y_train, alpha=0.01, epochs=1000):
  w = np.zeros((X.shape[1], 1))
  b = 0.0
  losses = []
  for i in range(epochs):
    dw, db = gradient(X_train_scaled, y_train, w, b)
    w -= alpha * dw
    b -= alpha * db
    if i % 100 == 0:
      y_hat_current = predict(X_train_scaled, w, b)
      losses.append(mse(y_hat_current, y_train))
  return w, b, np.array(losses)

what, bhat, losses_his = train_god(X_train_scaled, y_train, alpha=0.01, epochs=1000)
print(what, bhat, losses_his)

import matplotlib.pyplot as plt
import numpy as np

iterations = np.arange(len(losses_his)) * 100

plt.figure()
plt.plot(iterations, losses_his)
plt.xlabel("Iteration")
plt.ylabel("Cost (MSE)")
plt.title("Training Loss over Iterations")
plt.show()